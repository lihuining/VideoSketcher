
sd_version: "2.1"
#sd_version: "1.5"
input_path: "/media/allenyljiang/2CD8318DD83155F4/video_in_the_wild/12693267_1080_1920_30fps"
#work_dir: "outputs/"
work_dir: "/media/allenyljiang/2CD8318DD83155F4/CVPR2025/Struct_latents"

#app_image_path: "Dataset/Sketch_dataset/ref2sketch_yr/ref_camel/ref_0049.jpg"
app_image_path: "Dataset/Sketch_dataset/ref2sketch_yr/ref_camel/ref0030.jpg"
app_image_save_path: "Codes/cross-image-attention/data/" # 不需要加名字

height: 512
width: 512
## cross image attention
enable_edit: True
use_masked_adain: False
load_latents: True  # false则重新invert
keep_struct: False # inject structure
keep_struct_end: 61
gamma: 0.6 #0.6
gamma_end: 61
perform_cross_frame: False # 设置为True的时候chunk_size至少为2
perform_cross_frame_with_prev: False # 设置为True的时候chunk_size为1
#perform_cross_frame: False #
#perform_cross_frame_with_prev: True
latent_update: False
start_frame: 0
update_with_matching: False
update_with_matching_guidance: 1
swap_guidance_scale: 1.0
update_with_matching_start_time: 1
update_with_matching_end_time: 671
use_edge: False
contrast_strength: 1
use_adaptive_contrast: False
inversion:
  save_path: "${work_dir}/latents"
#  prompt: "a dog walking on the ground near a bush."
  prompt: "a photo of an object"
  steps: 100 # inversion的时间步
  save_intermediate: True # Save intermediate latents. Required when using PnP.
  n_frames: 50 #2 #null #2 #50 #11 # 15 # 70
  force: True # Force run inversion even inverted latents have been found

generation:
  control: "pnp"
  chunk_size: 2  #
  guidance_scale: 7.5 # CFG scale
  n_timesteps: 50
  negative_prompt: "ugly, blurry, low res"
  prompt:
    VG: "a dog walking on the ground near a bush, Van Gogh style."
    desert: "a dog walking in the desert near a bush."


  latents_path: "${work_dir}/latents"
  output_path: "${work_dir}"

  # VidToMe args. See details in "src/vidtome/patch.py, apply_patch()"
  local_merge_ratio: 0.9
  global_merge_ratio: 0.8
  global_rand: 0.5
  
seed: 123
device: "cuda"
base_config: "configs/default.yaml" # Use default values in template.yaml

float_precision: "fp32"
enable_xformers_memory_efficient_attention: True

ablation_key: "gamma_end"
